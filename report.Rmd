---
documentclass: report
link-citations: true
urlcolor: blue
fontsize: 12pt
linestretch: 1.15
papersize: a4
geometry: margin=25mm,top=10mm
output:
  bookdown::pdf_document2:
    toc: true
    toc_depth: 3
    includes:
      in_header: preamble.tex
    extra_dependencies: ["bm"]
title: |
  | Computational Statistics
  | Project 2 \vspace{1cm}
  | ![](figures/sigillum-lmu.png){width=3cm} \vspace{1cm}
subtitle: |
  | The code for this project is available under https://github.com/max607/computational-statistics-em.
author: |
  | By
  | Maximilian Schneider
date: "`r format(Sys.time(), '%d.%m.%Y')`"
---

<style>
body {
text-align: justify}
.main-container {
  max-width: 1000px;
  margin-left: auto;
  margin-right: auto;
}
</style>

```{r setup, include=FALSE}
# rmarkdown settings
knitr::opts_chunk$set(fig.align = "center", out.width = '80%', echo = FALSE, message = FALSE,
                      warning = FALSE)

# packages
library(magrittr)
library(data.table)
library(ggplot2); theme_set(theme_bw())

# code
source("R/code.R")
```

# Maximum Likelihood estimation of $\theta$

The pdf is

\begin{equation}
  f(y_i; \theta) = \frac{\theta^2}{\theta + 1} (1 + y_i) \exp(-\theta y_i), i = 1 \dots n.
\end{equation}

The log likelihood is

\begin{align}
  \ell(\theta) &= 2n \log(\theta) - n \log(\theta + 1) - \theta \sum_{i = 1}^n y_i + c \\
               &\propto 2 \log(\theta) - \log(\theta + 1) - \theta \bar{y} + c.
\end{align}

The first derivative is

\begin{equation}
  \ell'(\theta) = \frac{2}{\theta} - \frac{1}{\theta + 1} - \bar{y},
\end{equation}

where $\bar{y}$ is the sample mean of $\bm{y}$.
Note, we can drop the n. \
Setting the derivative to zero leads to equation \@ref(eq:equation1) which has to be solved for $\theta$.

\begin{equation}
  \frac{\theta + 2}{\theta (\theta + 1)} = \bar{y}.
  (\#eq:equation1)
\end{equation}

One approach is to use Newton-Raphson, which requires the second order derivative.

\begin{equation}
  \ell''(\theta) = -\frac{2}{\theta^2} + \frac{1}{(\theta + 1)^2},
\end{equation}

# Estimation of standard error

The pdf can be restated as

\begin{equation}
  f(y_i; \theta) = \frac{\theta}{\theta + 1} \theta \exp(-\theta y_i) + \frac{1}{\theta + 1} \theta^2 y_i \exp(-\theta y_i),
\end{equation}

i.e., a mixture of two gamma distributions in shape and rate parameterization.
$\theta$ is the rate and the shapes are equal to 1 and 2.

It is straight forward to simulate from this, but starting from $U \overset{iid}{\sim} U(0, 1)$ exponentially distributed variables can be obtained via inversion

\begin{equation}
  f^{-1}(u; \theta) = - \frac{\log(u)}{\theta},
\end{equation}

which is the same as a gamma with shape one and a gamma with shape 2 is obtained via the sum of 2 exponentials.
For optimizing computation time $n$ observations are generated in the following way:

1) Draw the number of shape 2 gammas (n2) by counting the number of $u < \frac{1}{\theta + 1}$
1) Sample $n$ and $n2$ uniforms
1) Transform the uniforms to exponentials using $f^{-1}$
1) Add to the fist $n2$ of the $n$ exponentials the other exponentials
1) Return

The result is a sample with $n2$ observations of a gamma distribution with shape 2 and $n - n2$ observation of a gamma distribution with shape 1.

For the purpose of estimating $\theta$ with the estimator of section \@ref(maximum-likelihood-estimation-of-theta) it is of no importance that the sample is sorted by shape.

# EM

Consider the more complex pdf

\begin{align}
  f(y_i; \theta, \lambda, \pi) = \pi \frac{\theta^2}{\theta + 1} (1 + y_i) \exp(-\theta y_i) + (1 - \pi) \lambda \exp(\lambda -y_i), \\
  y_i, \theta, \lambda \in \mathbb{R}^+, \pi \in [0, 1].
\end{align}

The goal is to estimate $\theta, \lambda$ and $\pi$ applying EM.
Because (3.1) is a mixture of two PDFs, one can rewrite the observed PDF introducing missing data

\begin{equation}
  \mathcal{L}_i(\theta, \lambda | x_i, y_i) = f_\theta(y_i)^{x_i} f_\lambda(y_i)^{1 - x_i},
\end{equation}

where $X_i \sim Ber(\pi)$.
We continue by applying the Bayes theorem

\begin{equation}
  f(x_i | \theta, \lambda, y_i) =
  \dfrac{\mathcal{L}_i(\theta, \lambda | x_i, y_i) f(x_i)}{f(\theta, \lambda)}.
\end{equation}

Note that $f(\theta, \lambda) = f(x_i = 0) \mathcal{L}_i(\theta, \lambda | x_i = 0, y_i) + f(x_i = 1) \mathcal{L}_i(\theta, \lambda | x_i = 1, y_i)$.
Via substituting we arrive at

\begin{equation}
  f(x_i | \theta, \lambda, y_i) =
  \dfrac{f_\theta(y_i)^{x_i} f_\lambda(y_i)^{1 - x_i} \pi^{x_i} (1 - \pi)^{1 - x_i}}{(1 - \pi) f_\lambda(y_i) + \pi f_\theta(y_i)}.
\end{equation}

From this we can read

\begin{equation}
  \mathbb{E}(X_i | \theta, \lambda, y_i) = \dfrac{f_\theta(y_i) \pi }{(1 - \pi) f_\lambda(y_i) + \pi f_\theta(y_i)}.
\end{equation}

Now we have everything we need: a approachable likelihood for the maximization step and the expectation of the missing data given the parameter estimates.
All parameters are independent, so the relevant parts of the likelihoods are

\begin{align}
  \ell(\pi | \bm{x}) &= \log(\pi) \sum_{i = 1}^n x_i + \log(1 - \pi) \sum_{i = 1}^n (1 - x_i) \\
  \ell(\lambda | \bm{x}, \bm{y}) &= \log(\lambda) \sum_{i = 1}^n (1 - x_i) - \lambda \sum_{i = 1}^n (1 - x_i) y_i + c \\
  \ell(\theta | \bm{x}, \bm{y}) &= 2 \log(\theta) \sum_{i = 1}^n x_i - \log(\theta + 1) \sum_{i = 1}^n x_i - \theta \sum_{i = 1}^n x_i y_i + c \\
  &\propto 2 \log(\theta) - \log(\theta + 1)  - \theta \tilde{y} + c
\end{align}

the augmented data contains all the information about $\pi$.
So, noting the distribution of $X_i$, we can maximize (3.7) to obtain the ML estimate $\hat{\pi} = \frac{1}{n} \sum_{i = 1}^n x_i$. \
It is also possible to solve (3.8) analytically.
The likelihood is maximized by $\hat{\lambda} = (n - \sum_{i = 1}^n x_i) / (\sum_{i = 1}^n y_i - \sum_{i = 1}^n x_i y_i)$. <!-- TODO: Probably wrong and the inverse is right -->
$\tilde{y} = \sum_{i = 1}^n x_i y_i / \sum_{i = 1}^n x_i$

Terminating condition

