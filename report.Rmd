---
documentclass: report
link-citations: true
urlcolor: blue
fontsize: 12pt
linestretch: 1.15
papersize: a4
geometry: margin=25mm,top=10mm
output:
  bookdown::pdf_document2:
    toc: true
    toc_depth: 3
    includes:
      in_header: preamble.tex
    extra_dependencies: ["bm"]
title: |
  | Computational Statistics
  | Project 2 \vspace{1cm}
  | ![](figures/sigillum-lmu.png){width=3cm} \vspace{1cm}
subtitle: |
  | The code for this project is available under https://github.com/max607/computational-statistics-em.
author: |
  | By
  | Maximilian Schneider
date: "`r format(Sys.time(), '%d.%m.%Y')`"
---

<style>
body {
text-align: justify}
.main-container {
  max-width: 1000px;
  margin-left: auto;
  margin-right: auto;
}
</style>

```{r setup, include=FALSE}
# rmarkdown settings
knitr::opts_chunk$set(fig.align = "center", out.width = '80%', echo = FALSE, message = FALSE,
                      warning = FALSE)

# packages
library(magrittr)
library(data.table)
library(ggplot2); theme_set(theme_bw())

# code
source("R/code.R")
```

# Maximum Likelihood estimation of $\theta$

The pdf is

\begin{equation}
  f(y_i; \theta) = \frac{\theta^2}{\theta + 1} (1 + y_i) \exp(-\theta y_i), i = 1 \dots n.
\end{equation}

The log likelihood is

\begin{align}
  \ell(\theta) &= 2n \log(\theta) - n \log(\theta + 1) - \theta \sum_{i = 1}^n y_i + c \\
               &\propto 2 \log(\theta) - \log(\theta + 1) - \theta \bar{y} + c.
\end{align}

The first derivative is

\begin{equation}
  \ell'(\theta) = \frac{2}{\theta} - \frac{1}{\theta + 1} - \bar{y},
\end{equation}

where $\bar{y}$ is the sample mean of $\bm{y}$.
Note, we can drop the n. \
Setting the derivative to zero leads to equation \@ref(eq:equation1) which has to be solved for $\theta$.

\begin{equation}
  \frac{\theta + 2}{\theta (\theta + 1)} = \bar{y}.
  (\#eq:equation1)
\end{equation}

One approach is to use Newton-Raphson, which requires the second order derivative.

\begin{equation}
  \ell''(\theta) = -\frac{2}{\theta^2} + \frac{1}{(\theta + 1)^2},
\end{equation}

# Estimation of standard error

The pdf can be restated as

\begin{equation}
  f(y_i; \theta) = \frac{\theta}{\theta + 1} \theta \exp(-\theta y_i) + \frac{1}{\theta + 1} \theta^2 y_i \exp(-\theta y_i),
\end{equation}

i.e., a mixture of two gamma distributions in shape and rate parameterization.
$\theta$ is the rate and the shapes are equal to 1 and 2.

It is straight forward to simulate from this, but starting from $U \overset{iid}{\sim} U(0, 1)$ exponentially distributed variables can be obtained via inversion

\begin{equation}
  f^{-1}(u; \theta) = - \frac{\log(u)}{\theta},
\end{equation}

which is the same as a gamma with shape one and a gamma with shape 2 is obtained via the sum of 2 exponentials.
For optimizing computation time $n$ observations are generated in the following way:

1) Draw the number of shape 2 gammas (n2) by counting the number of $u < \frac{1}{\theta + 1}$
1) Sample $n$ and $n2$ uniforms
1) Transform the uniforms to exponentials using $f^{-1}$
1) Add to the fist $n2$ of the $n$ exponentials the other exponentials
1) Return

The result is a sample with $n2$ observations of a gamma distribution with shape 2 and $n - n2$ observation of a gamma distribution with shape 1.

For the purpose of estimating $\theta$ with the estimator of section \@ref(maximum-likelihood-estimation-of-theta) it is of no importance that the sample is sorted by shape.

# EM

Consider the more complex pdf

\begin{align}
  f(y_i; \theta, \lambda, \pi) = \pi \frac{\theta^2}{\theta + 1} (1 + y_i) \exp(-\theta y_i) + (1 - \pi) \lambda \exp(\lambda -y_i), \\
  y_i, \theta, \lambda \in \mathbb{R}^+, \pi \in [0, 1].
\end{align}

The goal is to estimate $\theta, \lambda$ and $\pi$ applying EM.
As all observations are independent we formulate the complete likelihood as

\begin{equation}
  \mathcal{L}(\theta, \lambda, \pi | \bm{x}, \bm{y}) = \prod_{i = 1}^n (\pi \frac{\theta^2}{\theta + 1} (1 + y_i) \exp(-\theta y_i))^{x_i} + ((1 - \pi) \lambda \exp(\lambda -y_i))^{1 - x_i},
\end{equation}

with missing data $\bm{x}$.
As parameters are independent the relevant loglikelihoods parts are

* $\ell(\theta | \bm{x}, \bm{y}) = 2 \log(\theta) \sum_{i = 1}^n x_i - \log(\theta + 1) \sum_{i = 1}^n x_i - \theta \sum_{i = 1}^n x_i y_i + c$

* $\ell(\lambda | \bm{x}, \bm{y}) = \log(\lambda) \sum_{i = 1}^n (1 - x_i) - \lambda \sum_{i = 1}^n (1 - x_i) y_i + c$
<!-- * $\ell(\pi | \bm{x}, \bm{y}) = \log(\pi) \sum_{i = 1}^n x_i + \log(1 - \pi) \sum_{i = 1}^n (1 - x_i) + c$ -->

$\theta$ can be estimated as in section \@ref(maximum-likelihood-estimation-of-theta) replacing $\bar{y}$ with $(\sum_{i = 1}^n x_i y_i) / \sum_{i = 1}^n x_i$.
The derivative of $\ell(\lambda | \bm{x}, \bm{y})$ is

\begin{equation}
  \ell'(\lambda | \bm{x}, \bm{y}) = \frac{1}{\lambda} (n - \sum_{i = 1}^n x_i) - \sum_{i = 1}^n y_i + \sum_{i = 1}^n x_i y_i.
\end{equation}

This can be solved analytically

\begin{equation}
  \hat{\lambda} = \frac{n - \sum_{i = 1}^n x_i}{\sum_{i = 1}^n y_i - \sum_{i = 1}^n x_i y_i}
\end{equation}

